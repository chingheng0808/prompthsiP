<!DOCTYPE html>
<html lang="zh-Hant">
<head>
  <meta charset="UTF-8">
  <title><b>PromptHSI:</b> Universal Hyperspectral Image Restoration with Vision-Language Modulated Frequency Adaptation</title>
  <link href="style.css" rel="stylesheet" type="text/css" />
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
</head>
<body>
  <header>
    <h1>PromptHSI:</h1>
    <h1>Universal Hyperspectral Image Restoration with Vision-Language Modulated Frequency Adaptation</h1>
    <h2><a href="https://ming053l.github.io/" style="text-decoration: none;">Chia-Ming Lee<sup>1</sup></a>, <a href="https://scholar.google.com/citations?user=2UmoEfcAAAAJ&hl=zh-TW" style="text-decoration: none;">Ching-Heng Cheng<sup>1</sup></a>, <a href="https://vanlinlin.github.io/" style="text-decoration: none;">Yu-Fan Lin<sup>1</sup></a>, <a href="https://scholar.google.com.tw/citations?hl=zh-TW&view_op=list_works&gmla=AL3_zihMkuibR4LNjVHu_kgdxNASDeqOgpk6WdtWbMqh_9Li88mia0ilpTPOmU8ZVvhaFbhQc9MZxSy8NNWA34MybUc&user=HFImtzUAAAAJ" style="text-decoration: none;">Yi-Ching Cheng<sup>1</sup></a>, <a href="https://scholar.google.com.tw/citations?hl=zh-TW&user=pD-HgBYAAAAJ&view_op=list_works&gmla=AL3_zigkGgH0LwxSKUG4Q1g4Ivld056uxUH1cjieMkWLSUmQrLMVAHx_Rx4PYSY8dHLOEdwah0TPr6FwDQVhqZhrhIh1yD34Ig3klXaP5Aw" style="text-decoration: none;">Wo-Ting Liao<sup>1</sup></a>, <a href="https://cchsu.info/wordpress/" style="text-decoration: none;">Chih-Chung Hsu<sup>1</sup></a>, <a href="https://fuenyang1127.github.io/" style="text-decoration: none;">Fu-En Yang<sup>2,3</sup></a>, <a href="https://vllab.ee.ntu.edu.tw/ycwang.html" style="text-decoration: none;">Yu-Chiang Frank Wang<sup>2,3</sup></a></h2>
    <h2>Institute of Data Science, National Cheng Kung University<sup>1</sup></h2>
    <h2>Graduate Institute of Communication Engineering, National Taiwan University<sup>2</sup> NVIDIA<sup>3</sup></h2>
  </header>

  <div>
    <a target="_blank" href="https://arxiv.org/abs/2411.15922" class="btn btn-success">arXiv</a>
    <a target="_blank" href="https://drive.google.com/file/d/1SxrQiqa2sy3QMYLaoiDzeaXxrLfT-fES/view" class="btn btn-success">Supplementary</a>
    <a target="_blank" href="https://github.com/chingheng0808/PromptHSI" class="btn btn-success">Code</a>
    <a target="_blank" href="https://drive.google.com/drive/folders/1O0GDzoPt3AVD4mjXeu3R_lxyuTDEWWW1" class="btn btn-success">Dataset</a>
    <a target="_blank" href="https://drive.google.com/file/d/1H7NlKOLFKuN-wqGAizGKokydr4VuJ-Gg/view" class="btn btn-success">Model</a>
  </div>

  <div class="figure" style="width: 40%;" >
    <img src="figures/Comparison.jpg">

    <p style="text-align: left">Performance comparison between All-in-One and HSI restoration methods on the AVIRIS dataset. The proposed <strong>PromptHSI</strong> offers leading performance and controllability while retaining a compact model size.</p>

  </div>

  <div class="content">
    <h2><b>Abstract</b></h2>
    <p>
      Recent advances in All-in-One (AiO) RGB image restoration have demonstrated the effectiveness of prompt learning in handling multiple degradations within a single model. However, extending these approaches to hyperspectral image (HSI) restoration is challenging due to the domain gap between RGB and HSI features, information loss in visual prompts under severe composite degradations, and difficulties in capturing HSI-specific degradation patterns via text prompts. In this paper, we propose <strong>PromptHSI</strong>, the first universal AiO HSI restoration framework that addresses these challenges. By incorporating frequency-aware feature modulation—which utilizes frequency analysis to narrow down the restoration search space—and employing vision-language model (VLM)-guided prompt learning, our approach decomposes text prompts into intensity and bias controllers that effectively guide the restoration process while mitigating domain discrepancies. Extensive experiments demonstrate that our unified architecture excels at both fine-grained recovery and global information restoration across diverse degradation scenarios, highlighting its significant potential for practical remote sensing applications. The source code will be made publicly available.
    </p>

    <div class="figure" >
      <img src="figures/freq-analysis.png">
  
      <p style="text-align: center; font-size: 0.9em"><b>A diagram illustrating our proposed strategy</b>, where integrating prompt guidance with frequency modulation facilitates the adaptive selection of &lambda; and &mu;, enhancing HSI restoration under various degradation types. This mechanism encourages the model to efficiently converge toward optimal parameters.</p>
  
    </div>

    <div class="figure">
      <img src="figures/arch.png">
  
      <p style="text-align: center; font-size: 0.9em"><b>The architecture of the proposed PromptHSI</b>, which is designed to tackle the composite degradations within a single universal model.</p>
  
    </div>

    <div class="figure">
      <img src="figures/vis.png">
  
      <p style="text-align: center; font-size: 0.9em"><b>Visualization results for HSI restoration methods.</b> top: the reconstructed HSI, bottom: the residual image derived from Ground-truth and reconstructed result. Most of state-of-the-arts HSI restoration methods fail to deal with composite degradations.</p>
  
    </div>

    <div class="figure">
      <img src="figures/table.png">
  
      <p style="text-align: center; font-size: 0.9em"><b>Overall performance evaluation for composite degradation and complexity comparison.</b></p>
  
    </div>
  </div>

  <h5><b>If you find our work helpful, please consider citing the following:</b></h5>
  <div class="bibtex-container">
    <div class="bibtex-title">BibTeX</div>
    <pre class="bibtex-code" style="text-align: left;">
      @misc{lee2025prompthsiuniversalhyperspectralimage,
        title={PromptHSI: Universal Hyperspectral Image Restoration with Vision-Language Modulated Frequency Adaptation}, 
        author={Chia-Ming Lee and Ching-Heng Cheng and Yu-Fan Lin and Yi-Ching Cheng and Wo-Ting Liao and Fu-En Yang and Yu-Chiang Frank Wang and Chih-Chung Hsu},
        year={2025},
        eprint={2411.15922},
        archivePrefix={arXiv},
        primaryClass={eess.IV},
        url={https://arxiv.org/abs/2411.15922}, 
      }
    </pre>
  </div>
</body>
</html>
